{"cells":[{"cell_type":"markdown","metadata":{"id":"DS81hua9OHd9"},"source":["# mlflow - Experiment Tracking: Air Quality Classification"]},{"cell_type":"markdown","metadata":{"id":"Uoy_JdHiMpK0"},"source":["Our goal in this notebook is to demonstrate the power of **experiment tracking**  using **MLflow**‚Äîan open-source platform designed to simplify the management of machine learning workflows. In the world of machine learning, tracking experiments is crucial for ensuring transparency, reproducibility, and effective model management. MLflow allows us to log not only parameters, metrics, and models but also track and compare multiple experiments over time. This makes it easier to optimize model performance, fine-tune hyperparameters, and maintain a history of all experiments for future reference.\n","\n","In this notebook, we will use MLflow to track and manage the development of a classification model aimed at predicting **air quality**. Our dataset contains various environmental features, including temperature, humidity, pollutant levels (such as PM2.5, PM10, NO2, SO2, CO), and factors like proximity to industrial areas and population density. By building a predictive model, we aim to classify air quality into categories like \"Good\" , \"Moderate\" or \"Hazardous\" based on these features. Throughout the process, we will log every aspect of the model training and evaluation, from hyperparameters to performance metrics, ensuring we have a clear and organized record of our work with MLflow."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0qFxgxeKOBMB"},"outputs":[],"source":["# import Libraries\n","import mlflow\n","import mlflow.sklearn\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eeoktt5XOcCv","outputId":"2fb94b4e-7a83-4868-fe55-e2e786f81030"},"outputs":[{"name":"stdout","output_type":"stream","text":["   Temperature  Humidity  PM2.5  PM10   NO2   SO2    CO  \\\n","0         29.8      59.1    5.2  17.9  18.9   9.2  1.72   \n","1         28.3      75.6    2.3  12.2  30.8   9.7  1.64   \n","2         23.1      74.7   26.7  33.8  24.4  12.6  1.63   \n","3         27.1      39.1    6.1   6.3  13.5   5.3  1.15   \n","4         26.5      70.7    6.9  16.0  21.9   5.6  1.01   \n","\n","   Proximity_to_Industrial_Areas  Population_Density Air Quality  \n","0                            6.3                 319    Moderate  \n","1                            6.0                 611    Moderate  \n","2                            5.2                 619    Moderate  \n","3                           11.1                 551        Good  \n","4                           12.7                 303        Good  \n"]}],"source":["# Load the dataset from a CSV file\n","df = pd.read_csv('air_quality.csv')  # Replace with the path to your CSV file\n","\n","# Check the first few rows of the dataset to understand its structure\n","print(df.head())\n","\n","# Feature columns and target variable\n","X = df.drop('Air Quality', axis=1)\n","y = df['Air Quality']\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YAKM1FD7OgKG"},"outputs":[],"source":["# Encode the target variable (Air Quality) using Label Encoding\n","label_encoder = LabelEncoder()\n","y_encoded = label_encoder.fit_transform(y)\n","\n","# Standardize the features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3HZ0f3L8OiIH"},"outputs":[],"source":["# Split the dataset into training and testing sets (80% training, 20% testing)\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WD79ugSM1tQN"},"outputs":[],"source":["# Define model hyperparameters\n","params = {\n","    \"solver\": \"lbfgs\",\n","    \"max_iter\": 1000,\n","    \"random_state\": 42,\n","}\n","\n","# Train the model\n","lr = LogisticRegression(**params)\n","lr.fit(X_train, y_train)\n","\n","# Predict on the test set\n","y_pred = lr.predict(X_test)\n","\n","# Calculate evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred, average=\"weighted\")\n","recall = recall_score(y_test, y_pred, average=\"weighted\")\n","f1 = f1_score(y_test, y_pred, average=\"weighted\")\n"]},{"cell_type":"markdown","metadata":{"id":"CBmPlppoOlQJ"},"source":["### Experiment Tracking\n"]},{"cell_type":"markdown","metadata":{"id":"1eBrQD7AO5wJ"},"source":["Before we begin training our model, the first thing we need to do is define and set an experiment in MLflow.\n","\n","- **Experiment:** In MLflow, an experiment acts as a container that holds all the runs (individual model training sessions) related to a specific task or project.\n","- **Run:** A run represents a single execution of a machine learning model, including its parameters, metrics, and results. Each time you train a model, MLflow logs this as a new run."]},{"cell_type":"markdown","metadata":{"id":"vgvjFlnbPRh9"},"source":["By organizing our runs under a specific experiment, we can easily track, compare, and organize all the different model runs in one place. This structure allows us to quickly evaluate the impact of different configurations, hyperparameters, or model versions over time"]},{"cell_type":"markdown","metadata":{"id":"-tg19DEm1tQS"},"source":["- The **MLflow Tracking Server (UI)** is running on http://127.0.0.1:5000/.\n","By setting `mlflow.set_tracking_uri(remote_server_uri)`, you're telling MLflow to track experiments and logs on that server instead of using the default local storage.\n","- If you don't explicitly set a remote tracking URI with mlflow.set_tracking_uri(), MLflow will log the experiments and their associated metrics and models locally in the **./mlruns** directory.\n","This folder is where MLflow stores all the experiment data.\n","\n","    - **Experiment Folder:** Each experiment is assigned a unique experiment ID and stored in a subfolder within mlruns.\n","    - **Run Folder:** Within each experiment, every individual model training session is represented by a run, which is given a unique run ID. Each run is stored in its own folder under the experiment directory."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kEtCC-Do1tQT","outputId":"426f66c5-7d69-4117-e449-c7aad01770fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["http://127.0.0.1:5000/\n"]}],"source":["remote_server_uri = \"http://127.0.0.1:5000/\"\n","mlflow.set_tracking_uri(remote_server_uri)\n","print(mlflow.tracking.get_tracking_uri())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ImfE_0xOkN5","outputId":"daddca46-bdaa-4a7c-e597-2e4767ffcc66"},"outputs":[{"name":"stderr","output_type":"stream","text":["2025/03/07 23:54:34 INFO mlflow.tracking.fluent: Experiment with name 'Air_Quality_Experiment' does not exist. Creating a new experiment.\n"]},{"data":{"text/plain":["<Experiment: artifact_location='mlflow-artifacts:/426445182631609032', creation_time=1741384474131, experiment_id='426445182631609032', last_update_time=1741384474131, lifecycle_stage='active', name='Air_Quality_Experiment', tags={}>"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# Set the experiment name\n","mlflow.set_experiment('Air_Quality_Experiment')"]},{"cell_type":"markdown","metadata":{"id":"MK-j3cFsQYW7"},"source":["Once the experiment is set up in MLflow, we can start tracking various aspects of our model, such as **parameters, metrics, and the model itself**. These are logged during each run, allowing us to monitor and evaluate different model configurations over time. Here‚Äôs how we can log each of these elements:\n","\n","- `set_tag:` Used to set metadata for the run, such as specifying the model type, dataset name, or experiment category. This helps in organizing and filtering runs.\n","- `log_param:` This function is used to log parameters that are part of the model, such as hyperparameters (e.g., the number of iterations, learning rate, etc.).\n","- `log_metric:` This function logs performance metrics, such as accuracy, precision, recall, or loss, which are useful for evaluating how well the model is performing.\n","- `log_model:` This function is used to log the trained machine learning model, so it can be saved and retrieved for future use or deployment."]},{"cell_type":"markdown","source":["#### Logging parameters and Model"],"metadata":{"id":"f9O51JGFZ7jl"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nTaFatVFHOoX","outputId":"5f60689d-c1b3-4464-c1e4-06decfa88413"},"outputs":[{"name":"stderr","output_type":"stream","text":["2025/03/08 00:03:57 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy: 0.9470\n","Precision: 0.9479\n","Recall: 0.9470\n","F1 Score: 0.9470\n","Model saved in run: 3c566ec0bc6b488a88bb885d5a59c024\n","üèÉ View run blushing-cub-528 at: http://127.0.0.1:5000/#/experiments/426445182631609032/runs/3c566ec0bc6b488a88bb885d5a59c024\n","üß™ View experiment at: http://127.0.0.1:5000/#/experiments/426445182631609032\n"]}],"source":["# Start MLflow run\n","with mlflow.start_run():\n","\n","    # Set experiment tags\n","    mlflow.set_tag(\"model_type\", \"LogisticRegression\")\n","    mlflow.set_tag(\"experiment_name\", \"Air Quality Classification\")\n","\n","    # Log model parameters\n","    for param, value in params.items():\n","        mlflow.log_param(param, value)\n","\n","    # Log evaluation metrics\n","    mlflow.log_metric(\"accuracy\", accuracy)\n","    mlflow.log_metric(\"precision\", precision)\n","    mlflow.log_metric(\"recall\", recall)\n","    mlflow.log_metric(\"f1_score\", f1)\n","\n","    # Log trained model\n","    mlflow.sklearn.log_model(lr, \"model\")\n","\n","    # Print run details\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall: {recall:.4f}\")\n","    print(f\"F1 Score: {f1:.4f}\")\n","    print(f\"Model saved in run: {mlflow.active_run().info.run_id}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"slhlNgW0SQEy","outputId":"42250ac9-b545-4204-d184-e1aec5e414bd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Index(['run_id', 'experiment_id', 'status', 'artifact_uri', 'start_time',\n","       'end_time', 'metrics.accuracy', 'metrics.recall', 'metrics.f1_score',\n","       'metrics.precision', 'params.random_state', 'params.max_iter',\n","       'params.solver', 'tags.mlflow.source.name', 'tags.experiment_name',\n","       'tags.mlflow.log-model.history', 'tags.mlflow.source.type',\n","       'tags.mlflow.user', 'tags.mlflow.runName', 'tags.model_type'],\n","      dtype='object')\n","Run ID: 3c566ec0bc6b488a88bb885d5a59c024, Status: FINISHED, Accuracy: 0.947\n","Run ID: c7525f4847cf46248ebbb98a6c06018f, Status: FINISHED, Accuracy: 0.948\n"]}],"source":["# Get the experiment ID\n","experiment_name = 'Air_Quality_Experiment'\n","experiment = mlflow.get_experiment_by_name(experiment_name)\n","experiment_id = experiment.experiment_id if experiment else None\n","\n","if experiment_id:\n","    # Get the list of runs in the experiment\n","    runs = mlflow.search_runs(experiment_ids=experiment_id)\n","\n","    # Print run details\n","    print(runs.columns)\n","\n","    # Access metrics.accuracy correctly\n","    for index, row in runs.iterrows():\n","        print(f\"Run ID: {row['run_id']}, Status: {row['status']}, Accuracy: {row['metrics.accuracy']}\")\n","else:\n","    print(f\"Experiment with name '{experiment_name}' not found.\")"]},{"cell_type":"markdown","source":["#### Loading the Model for Prediciton"],"metadata":{"id":"QmvThahqaDMU"}},{"cell_type":"markdown","metadata":{"id":"0jBRuvSu1tQZ"},"source":["After a model is logged in MLflow, it can be loaded again by specifying the **run ID**. The model is retrieved using `mlflow.sklearn.load_model()`, which takes the run ID to locate and load the specific model from the logged experiment. This allows you to access and use the model at any time after it has been logged, facilitating its reuse in future predictions or analysis."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bXG_oLER_3YM"},"outputs":[],"source":["# Logged model in MLFlow\n","mlflow_run_id = 'c7525f4847cf46248ebbb98a6c06018f' #choose the runid for the model you want to load\n","logged_model_path = f\"runs:/{mlflow_run_id}/model\"\n","\n","# Load model as a sklearn model\n","loaded_model = mlflow.sklearn.load_model(logged_model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PNlUsn_e1tQZ","outputId":"638abe9f-28b7-4824-cfcb-11ef87a9b9fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.948\n"]}],"source":["y_pred = loaded_model.predict(X_test)\n","\n","# Calculate evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","print(accuracy)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"mlflow-env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.21"}},"nbformat":4,"nbformat_minor":0}